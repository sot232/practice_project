{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Chatbot from Scratch in Python (using NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction: https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jeong-\n",
      "[nltk_data]     ugim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jeong-\n",
      "[nltk_data]     ugim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('chatbot.txt', 'r', errors='ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to list of sentences\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "# converts to list of words\n",
    "word_tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the raw text\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) \n",
    "                         for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword matching\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\",\n",
    "                   \"what's up\", \"hey\",)\n",
    "\n",
    "GREETING_RESPOSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\",\n",
    "                     \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPOSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Response\n",
    "\n",
    "# To convert a collection of raw documents to a matrix of TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Cosine similarity module\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# we will find the similarity between words entered by the user\n",
    "# and the words in the corpus\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize,\n",
    "                              stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if (req_tfidf==0):\n",
    "        robo_response = robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots.      If you want to exit, type Bye!\n",
      "What is ELIZA?\n",
      "ROBO: what is eliza?\n",
      "yes\n",
      "ROBO: I am sorry! I don't understand you\n",
      "Describe chatbot design?\n",
      "ROBO: [3][4] chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, hr, marketing, news, personal, productivity, shopping, social, sports, travel and utilities.\n",
      "Who was Alan Turing?\n",
      "ROBO: [5]\n",
      "\n",
      "\n",
      "contents\n",
      "1\tbackground\n",
      "2\tdevelopment\n",
      "3\tapplication\n",
      "3.1\tmessaging apps\n",
      "3.1.1\tas part of company apps and websites\n",
      "3.2\tcompany internal platforms\n",
      "3.3\ttoys\n",
      "3.4\tchatbots in medicine and for mental health\n",
      "4\tchatbot development platforms\n",
      "5\tmalicious use\n",
      "6\tsee also\n",
      "7\tcitations\n",
      "8\treferences\n",
      "background[edit]\n",
      "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published,[6] which proposed what is now called the turing test as a criterion of intelligence.\n",
      "ELIZA?\n",
      "ROBO: what is eliza?\n",
      "What is ELIZA?\n",
      "ROBO: what is eliza?\n",
      "Are chatbot's any good?\n",
      "ROBO: [22][23]\n",
      "\n",
      "a 2017 study showed 4% of companies used chatbots.\n",
      "Hi\n",
      "ROBO: hello\n",
      "What is a chatbot?\n",
      "ROBO: [33]\n",
      "\n",
      "hello barbie is an internet-connected version of the doll that uses a chatbot provided by the company toytalk,[34] which previously used the chatbot for a range of smartphone-based characters for children.\n",
      "Bye\n",
      "ROBO: Bye! taks care..\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots.\\\n",
    "      If you want to exit, type Bye!\")\n",
    "\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if (user_response!='bye'):\n",
    "        if (user_response == 'thanks' or user_response == 'thank you'):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if (greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \", end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! taks care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
