{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction: https://towardsdatascience.com/write-a-simple-summarizer-in-python-e9ca6138a08e\n",
    "\n",
    "# Summarizer\n",
    "\n",
    "## How the Summarizer Works\n",
    "1. Read from source — Read the unabridged content from the source, a file in the case of this exercise.\n",
    "2. Perform formatting and cleanup — Format and clean up our format so that it is free of extra white space or other issues.\n",
    "3. Tokenize input — Take the input and break it up into its individual words.\n",
    "4. Scoring — Score (count) the frequency of each word in the input and score sentences based on word score.\n",
    "5. Selection — Choose the top N sentences based on their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return file.read()\n",
    "    except IOError as e:\n",
    "        print(\"Fatal Error: File ({}) could not be located or is not readable.\".format(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitizing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    replace = {\n",
    "        ord('\\f') : ' ',\n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "    return data.translate(replace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom tokenize function\n",
    "We won't use this for now.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_content(content):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    return [\n",
    "        sent_tokenize(content),\n",
    "        [word for word in words if word not in stop_words]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tokens(filtered_words, sentence_tokens):\n",
    "    \"\"\"\n",
    "    In this case,\n",
    "    word_freq: <FreqDist with 209 samples and 373 outcomes>\n",
    "    word_freq stores a structure where each key is the word\n",
    "    and each value is the number of times that word occured.\n",
    "    ex. FreqDist({'the': 27, '.': 21, 'Greenland': 7, 'at': 6, ...})\n",
    "    \"\"\"\n",
    "    word_freq = FreqDist(filtered_words)\n",
    "    ranking = defaultdict(int)\n",
    "    for i, sentence in enumerate(sentence_tokens):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                ranking[i] += word_freq[word]\n",
    "    return ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(ranks, sentences):\n",
    "    \"\"\"\n",
    "    len is 4\n",
    "    summary length depends on this number\n",
    "    nlargest takes sentence ranking and\n",
    "    turns it into a list of the numeric positions of the sentences\n",
    "    in the sentence_tokens variables\n",
    "    \"\"\"\n",
    "    index = nlargest(3, ranks, key=ranks.get)\n",
    "    final_sentences = [sentences[j] for j in sorted(index)]\n",
    "    return ' '.join(final_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 0, 20, 5]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest(4, sentence_ranks, key=sentence_ranks.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = nlargest(4, sentence_ranks, key=sentence_ranks.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "14\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for j in sorted(index):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 218, 1: 93, 2: 102, 3: 114, 4: 36, 5: 140, 6: 30, 7: 79, 8: 77, 9: 128, 10: 74, 11: 46, 12: 77, 13: 54, 14: 237, 15: 137, 16: 126, 17: 111, 18: 78, 19: 82, 20: 204})\n"
     ]
    }
   ],
   "source": [
    "path = 'GreenlandIsMelting.txt'\n",
    "\n",
    "content = read_file(path)\n",
    "content = sanitize_input(content)\n",
    "\n",
    "sentence_tokens = sent_tokenize(content)\n",
    "word_tokens = word_tokenize(content)\n",
    "sentence_ranks = score_tokens(word_tokens, sentence_tokens)\n",
    "\n",
    "#print(sentence_tokens[0])\n",
    "#print(word_tokens)\n",
    "print(sentence_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 0, 20, 5]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest(4, sentence_ranks, key=sentence_ranks.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 218,\n",
       "             1: 93,\n",
       "             2: 102,\n",
       "             3: 114,\n",
       "             4: 36,\n",
       "             5: 140,\n",
       "             6: 30,\n",
       "             7: 79,\n",
       "             8: 77,\n",
       "             9: 128,\n",
       "             10: 74,\n",
       "             11: 46,\n",
       "             12: 77,\n",
       "             13: 54,\n",
       "             14: 237,\n",
       "             15: 137,\n",
       "             16: 126,\n",
       "             17: 111,\n",
       "             18: 78,\n",
       "             19: 82,\n",
       "             20: 204})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet. And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further. Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(sentence_ranks, sentence_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
